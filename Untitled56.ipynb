{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb5063d-6b19-4758-a6ec-89ce774b3e50",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of precision and recall in the context of classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9413b1d-af0c-4a84-9ae8-776dc82f220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision:\n",
    "\n",
    "Precision measures the proportion of correctly predicted positive instances (true positives) among all instances predicted as positive (true positives + false positives).\n",
    "It answers the question: \"Of all the instances predicted as positive, how many are truly positive?\"\n",
    "Mathematically, precision is calculated as:\n",
    "Precision\n",
    "=\n",
    "True Positives\n",
    "True Positives\n",
    "+\n",
    "False Positives\n",
    "Precision= \n",
    "True Positives+False Positives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "Recall (Sensitivity):\n",
    "\n",
    "Recall, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances (true positives) among all actual positive instances (true positives + false negatives).\n",
    "It answers the question: \"Of all the true positive instances, how many did the model correctly predict?\"\n",
    "Mathematically, recall is calculated as:\n",
    "Recall\n",
    "=\n",
    "True Positives\n",
    "True Positives\n",
    "+\n",
    "False Negatives\n",
    "Recall= \n",
    "True Positives+False Negatives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "Interpretation:\n",
    "\n",
    "Precision: A high precision indicates that the model has a low false positive rate, meaning that when it predicts an instance as positive, it is likely to be correct. Precision is important when the cost of false positives is high, and you want to minimize the number of incorrect positive predictions.\n",
    "Recall: A high recall indicates that the model has a low false negative rate, meaning that it correctly identifies most of the positive instances. Recall is important when the cost of false negatives is high, and you want to minimize the number of missed positive instances.\n",
    "Trade-off:\n",
    "\n",
    "Precision and recall are often inversely related: increasing precision may lead to a decrease in recall, and vice versa. This trade-off depends on the model's threshold for classifying instances as positive or negative. Adjusting this threshold can affect the precision and recall of the model.\n",
    "In summary, precision and recall are complementary metrics used to evaluate the performance of classification models, providing insights into the model's ability to correctly identify positive instances (precision) and capture all positive instances (recall). Choosing the appropriate metric depends on the specific requirements and priorities of the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a432528-cd7f-405f-9d87-9b7839cc417d",
   "metadata": {},
   "source": [
    "## Q2. What is the F1 score and how is it calculated? How is it different from precision and recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9158f-d722-4e86-a5cd-17063604cf82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
